{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nrMAcbkCYMX3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the wine dataset\n",
        "data_columns = [\n",
        "    \"Class\", \"Alcohol\", \"Malic_acid\", \"Ash\", \"Alcalinity_of_ash\",\n",
        "    \"Magnesium\", \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\",\n",
        "    \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \"OD280/OD315\", \"Proline\"\n",
        "]\n",
        "\n",
        "wine_data = pd.read_csv(\"wine.data\", header=None, names=data_columns)"
      ],
      "metadata": {
        "id": "oYUwGj47Y56n"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features and target\n",
        "X = wine_data.drop(\"Class\", axis=1)\n",
        "y = wine_data[\"Class\"]\n"
      ],
      "metadata": {
        "id": "bRdriwkKY9FX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the target\n",
        "ohe = OneHotEncoder(sparse_output=False)  # Ganti 'sparse' dengan 'sparse_output'\n",
        "y_encoded = ohe.fit_transform(y.values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "UozC9tGhY_lK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Zwf2OfX9ZN2s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "0WM56TN7ZPIw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "3RhLg01LZRc2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        activation_map = {\n",
        "            'linear': nn.Identity(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'relu': nn.ReLU(),\n",
        "            'softmax': nn.Softmax(dim=1),\n",
        "            'tanh': nn.Tanh()\n",
        "        }\n",
        "        for i, hidden_size in enumerate(hidden_layers):\n",
        "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_size))\n",
        "            layers.append(activation_map[activation_fn])\n",
        "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "6BgbwefeZUG2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment function\n",
        "def run_experiment(hidden_layers, activation_fn, epochs, learning_rate, batch_size):\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = y_train.shape[1]\n",
        "\n",
        "    model = MLP(input_size, hidden_layers, output_size, activation_fn)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        permutation = torch.randperm(X_train_tensor.size(0))\n",
        "        for i in range(0, X_train_tensor.size(0), batch_size):\n",
        "            indices = permutation[i:i + batch_size]\n",
        "            batch_X, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y.argmax(dim=1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        _, actual = torch.max(y_test_tensor, 1)\n",
        "        accuracy = (predicted == actual).sum().item() / y_test_tensor.size(0)\n",
        "    return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "uZM01kHAZWwy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Hidden Layers\n",
        "hidden_layer_configs = [[4], [8], [16], [32], [64], [4, 4], [8, 8], [16, 16], [32, 32], [64, 64]]\n",
        "print(\"Hidden Layers Experiment\")\n",
        "for hidden_layers in hidden_layer_configs:\n",
        "    accuracy = run_experiment(hidden_layers, 'relu', 50, 0.01, 32)\n",
        "    print(f\"Hidden Layers: {hidden_layers}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXoC4ocMZZI7",
        "outputId": "989fdfd0-fbfa-4ea9-b890-5ba3a91b45c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layers Experiment\n",
            "Hidden Layers: [4], Accuracy: 1.00\n",
            "Hidden Layers: [8], Accuracy: 1.00\n",
            "Hidden Layers: [16], Accuracy: 1.00\n",
            "Hidden Layers: [32], Accuracy: 1.00\n",
            "Hidden Layers: [64], Accuracy: 1.00\n",
            "Hidden Layers: [4, 4], Accuracy: 1.00\n",
            "Hidden Layers: [8, 8], Accuracy: 1.00\n",
            "Hidden Layers: [16, 16], Accuracy: 1.00\n",
            "Hidden Layers: [32, 32], Accuracy: 1.00\n",
            "Hidden Layers: [64, 64], Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Activation Functions\n",
        "activation_functions = ['linear', 'sigmoid', 'relu', 'softmax', 'tanh']\n",
        "print(\"\\nActivation Functions Experiment\")\n",
        "for activation_fn in activation_functions:\n",
        "    accuracy = run_experiment([32], activation_fn, 50, 0.01, 32)\n",
        "    print(f\"Activation Function: {activation_fn}, Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFeDek7nZdUs",
        "outputId": "cd3861fb-bc7c-45cc-f5d7-70d12b8d2c76"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Activation Functions Experiment\n",
            "Activation Function: linear, Accuracy: 1.00\n",
            "Activation Function: sigmoid, Accuracy: 1.00\n",
            "Activation Function: relu, Accuracy: 1.00\n",
            "Activation Function: softmax, Accuracy: 1.00\n",
            "Activation Function: tanh, Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Epochs\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]\n",
        "print(\"\\nEpochs Experiment\")\n",
        "for epochs in epochs_list:\n",
        "    accuracy = run_experiment([32], 'relu', epochs, 0.01, 32)\n",
        "    print(f\"Epochs: {epochs}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzuSqkBAZjnB",
        "outputId": "4e027b9d-bba7-4607-821c-328c6fd1306f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epochs Experiment\n",
            "Epochs: 1, Accuracy: 0.97\n",
            "Epochs: 10, Accuracy: 1.00\n",
            "Epochs: 25, Accuracy: 1.00\n",
            "Epochs: 50, Accuracy: 1.00\n",
            "Epochs: 100, Accuracy: 1.00\n",
            "Epochs: 250, Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Learning Rates\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "print(\"\\nLearning Rates Experiment\")\n",
        "for learning_rate in learning_rates:\n",
        "    accuracy = run_experiment([32], 'relu', 50, learning_rate, 32)\n",
        "    print(f\"Learning Rate: {learning_rate}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_CIiY6bce9-",
        "outputId": "f5ce7ddf-4619-4888-bf25-555bb2d8b269"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learning Rates Experiment\n",
            "Learning Rate: 10, Accuracy: 1.00\n",
            "Learning Rate: 1, Accuracy: 1.00\n",
            "Learning Rate: 0.1, Accuracy: 1.00\n",
            "Learning Rate: 0.01, Accuracy: 1.00\n",
            "Learning Rate: 0.001, Accuracy: 1.00\n",
            "Learning Rate: 0.0001, Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Batch Sizes\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "print(\"\\nBatch Sizes Experiment\")\n",
        "for batch_size in batch_sizes:\n",
        "    accuracy = run_experiment([32], 'relu', 50, 0.01, batch_size)\n",
        "    print(f\"Batch Size: {batch_size}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nAll experiments completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLKkXoWrcjym",
        "outputId": "32644daa-0818-40c1-e8bc-0e0fb3472017"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch Sizes Experiment\n",
            "Batch Size: 16, Accuracy: 1.00\n",
            "Batch Size: 32, Accuracy: 1.00\n",
            "Batch Size: 64, Accuracy: 1.00\n",
            "Batch Size: 128, Accuracy: 1.00\n",
            "Batch Size: 256, Accuracy: 1.00\n",
            "Batch Size: 512, Accuracy: 1.00\n",
            "\n",
            "All experiments completed.\n"
          ]
        }
      ]
    }
  ]
}